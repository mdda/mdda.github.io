---
layout: post
category: AI
title: On the Statistical Laws of Linguistic Distributions
tagline:  Paper 
date: 2015-04-10
tags: [Zipf,PowerLaw,NLP]
published: false
---
{% include JB/setup %}

{% include custom/paper_review %}


Belevitch 1959

Argument shows that rank proportional to frequency (to a power) is a 
characteristic of most roughly linear distributions.  


Perhaps truncated log-normal is an even better match to actual word frequencies, 
rather than defining distribution in terms of Zipf relationship.

i.e. fix up the distribution to be the tail of a log-normal distribution, and see what happens

But why use a tail?  Isn't there a distribution where the whole thing is relevant (exponential?)

    
  Statistics:
    Name distributions = ~Power Law
      file:///home/andrewsm/Downloads/Statistical%20distribution%20of%20Chinese%20names.pdf
    Least effort and the origins of scaling in human language
      http://www.ncbi.nlm.nih.gov/pmc/articles/PMC298679/
    Zipf overview (2014)
      http://colala.bcs.rochester.edu/papers/piantadosi2014zipfs.pdf
