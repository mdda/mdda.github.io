---
date: 2019-02-01
title:  "Deep Learning Ears"
tagline: Presentation
category: AI
tags:
- Presentation
- SpeechRecognition
- ASR
layout: post
published: false
---
{% include JB/setup %}



### Presentation Link

The slides for my talk are here :

<a href="http://redcatlabs.com/2018-11-12_GDG-SF_DeepLearningVoices/" target="_blank">
![Presentation Screenshot]({{ site.url }}/assets/img/2018-11-12_GDG-SF_DeepLearningVoices_600x390.png)
</a>

If there are any questions about the presentation please ask below, 
or contact me using the details given on the slides themselves.

<a href="http://redcatlabs.com/2018-11-12_GDG-SF_DeepLearningVoices/#/8/1" target="_blank">
![Presentation Content Example]({{ site.url }}/assets/img/2018-11-12_GDG-SF_DeepLearningVoices_8-1_600x390.png)
</a>

<!--

Tensor2Tensor library
  Notebook :
    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/asr_transformer.ipynb
  TPU instructions :
    https://cloud.google.com/tpu/docs/tutorials/automated-speech-recognition
  Comment by dev:
    There are many difference between v1 and v2 (actually transformer_librispeech_v2 is exactly transformer_librispeech)
    https://github.com/tensorflow/tensor2tensor/issues/896#issuecomment-400975458
  Issue : ASR Transformer performance vs. Google Speech-to-Text
    https://github.com/tensorflow/tensor2tensor/issues/1121
  Papers
    https://arxiv.org/abs/1712.01769  == https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46687.pdf
      = State-of-the-art Speech Recognition With Sequence-to-Sequence Models


Other libraries
  wav2letter++
    https://arxiv.org/abs/1812.07625v1
      = wav2letter++: The Fastest Open-source Speech Recognition System
  Kaldi

  Data
    https://github.com/juliagusak/dataloaders


LMs for ASR
  Papers
    http://homepages.inf.ed.ac.uk/miles/papers/emnlp07.pdf
      = Smoothed Bloom filter language models: Tera-Scale LMs on the Cheap
    https://arxiv.org/abs/1811.04284
      = Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling
    https://arxiv.org/abs/1808.02480
      = Deep context: end-to-end contextual speech recognition (refers to 1712.01769 as [13])
      
      
Related       
    https://arxiv.org/abs/1806.04558
      = Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis
      
      
https://github.com/zzw922cn/awesome-speech-recognition-speech-synthesis-papers      
http://www.arxiv-sanity.com/search?q=speech+recognition

https://arxiv.org/abs/1712.01769
https://arxiv.org/pdf/1808.02480.pdf
https://arxiv.org/pdf/1807.10857.pdf
https://arxiv.org/abs/1807.09597

Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis - Google
  https://arxiv.org/pdf/1806.04558.pdf

Fully Convolutional Speech Recognition - Collobert, Facebook
  https://arxiv.org/abs/1812.06864v1
  https://github.com/facebookresearch/wav2letter
  https://github.com/facebookresearch/fairseq
  prev?: 
    End-to-End Speech Recognition From the Raw Waveform  - Collobert, Facebook
      https://arxiv.org/abs/1806.07098v2

The PyTorch-Kaldi Speech Recognition Toolkit - Bengio, MILA
  https://arxiv.org/abs/1811.07453v1

A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition
  https://arxiv.org/abs/1807.10857v2

Attention-Based Models for Speech Recognition
  https://arxiv.org/pdf/1506.07503.pdf

An Online Attention-based Model for Speech Recognition
  https://arxiv.org/abs/1811.05247v1

https://arxiv.org/abs/1805.10387v2
https://arxiv.org/abs/1811.06621v1
https://arxiv.org/abs/1811.04531v1

  
Cycle-consistency training for end-to-end speech recognition
  https://arxiv.org/abs/1811.01690v1
  
Cascaded CNN-resBiLSTM-CTC: An End-to-End Acoustic Model For Speech Recognition
  https://arxiv.org/abs/1810.12001v2

Densely Connected Convolutional Networks for Speech Recognition
  https://arxiv.org/abs/1808.03570v1

Smaller models:
  Low-Dimensional Bottleneck Features for On-Device Continuous Speech Recognition
    https://arxiv.org/abs/1811.00006v1

!-->


PS:  And if you liked the content, please 'star' my <a href="https://github.com/mdda/deep-learning-workshop" target="_blank">Deep Learning Workshop</a> repo ::
<!-- From :: https://buttons.github.io/ -->
<!-- Place this tag where you want the button to render. -->
<span style="position:relative;top:5px;">
<a aria-label="Star mdda/deep-learning-workshop on GitHub" data-count-aria-label="# stargazers on GitHub" data-count-api="/repos/mdda/deep-learning-workshop#stargazers_count" data-count-href="/mdda/deep-learning-workshop/stargazers" data-icon="octicon-star" href="https://github.com/mdda/deep-learning-workshop" class="github-button">Star</a>
<!-- Place this tag right after the last button or just before your close body tag. -->
<script async defer id="github-bjs" src="https://buttons.github.io/buttons.js"></script>
</span>

