---
layout: post
category: AI
title: FitNets - Hints for Thin Deep Nets
tagline: Paper Takeaway
date: 2015-02-23
tags: [NeuralNetworks,PaperTakeaway]
published: false
---
{% include JB/setup %}

{% include custom/paper_review %}

This write-up contains a few takeaways that I had from the recent paper :
[FitNets: Hints for Thin Deep Nets (Romero et al 2015)](http://arxiv.org/abs/1412.6550).

Also, it is somewhat complementary to [my previous thoughts](/ai/2014/09/17/paper-deep-nets/).


Mid-stage layer of teacher used for hints to mid-stage layer of student.
* So, effectively, teacher is broken into sub-segments, similarly with student
* Basic operation is to narrow and deepen a single segment
  
One wrinkle : The 'regressor' layer is a learned mapping from the target student layer to
the corresponding teacher layer - which appears to be a fully-connected layer (including a
non-linearity), with accommodation made for the case when the corresponding layers are convolutional.

This regressor layer (after teacher-student training is complete) can be thrown away.

Instead of looking at the regressor layer as a bridge, one can also view it as 
an extra step in a two-sided step-ladder converging on the hint layer.
But for the non-linearity `r()`, the regressor would be something akin to PCA on the prior
step in the teacher.   There is probably something mathematically approachable in this viewpoint...


