---
date: 2022-01-07
title:  "Text Similarity for NLP"
tagline: Workshop
category: AI
tags:
- Workshop
- Machine Learning
- Transformers
- BERT
- SentenceBERT
layout: post
published: true
---
{% include JB/setup %}


### Workshop Link

Sam Witteveen and I were invited to give an NLP workshop 
by the [AI Professionals Association (AIP)](https://www.aip.org.sg/) 
which is "the first grassroot-driven Association for engineers and professionals 
working in AI-related roles founded in Singapore".

The workshop was part of an in-person event 
(which surprised us, but it was certainly nice to be able 
to meet some of our course participants face-to-face, whereas we'd only 
previously interacted with them via Zoom/Meet/Email/Colab), and part
of their first [AIP Developer Conference](https://www.aip.org.sg/event/aip-dev-conf-2022/).

The content of the workshop that we proposed 
included a tour through some of the latest techniques 
being used in research and industry for comparing and measuring 
text similarity for tasks such as 
Search, Conversational AI, Paraphrasing and Summarization. We 
also explored how a variety of Transformers are being used for these tasks 
and compare the advantages and disadvantages of using them.

In terms of audience, we presumed that participants had a decent understanding 
of NLP and Transformers before taking the workshop (i.e. this was not aimed at beginners).


The slides for my talk are here :

<a href="https://redcatlabs.com/2022-01-07_AIP_Workshop-NLP/#/intro" target="_blank">
![Presentation Screenshot]({{ site.url }}/assets/img/2022-01-07_AIP_Workshop-NLP_600x390.png)
</a>

If there are any questions about the presentation please ask below, 
or contact me using the details given on the slides themselves.

<a href="https://redcatlabs.com/2022-01-07_AIP_Workshop-NLP/#/12/1" target="_blank">
![Presentation Content Example]({{ site.url }}/assets/img/2022-01-07_AIP_Workshop-NLP_12-1_600x390.png)
</a>


>  In order to prepare these materials (and associated notebooks), we made use of 
>  Google Cloud - and their team generously provided us with some cloud credits to do so.  These
>  were particularly important because the Transformer-based models we were demonstrating
>  need beefy machines to train, and good hardware at inference time too.



PS:  And if you liked the content, please 'star' my <a href="https://github.com/mdda/deep-learning-workshop" target="_blank">Deep Learning Workshop</a> repo ::
<!-- From :: https://buttons.github.io/ -->
<!-- Place this tag where you want the button to render. -->
<span style="position:relative;top:5px;">
<a aria-label="Star mdda/deep-learning-workshop on GitHub" data-count-aria-label="# stargazers on GitHub" data-count-api="/repos/mdda/deep-learning-workshop#stargazers_count" data-count-href="/mdda/deep-learning-workshop/stargazers" data-icon="octicon-star" href="https://github.com/mdda/deep-learning-workshop" class="github-button">Star</a>
<!-- Place this tag right after the last button or just before your close body tag. -->
<script async defer id="github-bjs" src="https://buttons.github.io/buttons.js"></script>
</span>


