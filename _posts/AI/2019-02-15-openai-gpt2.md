---
date: 2019-02-15
title:  "OpenAI GPT2"
tagline: Commentary
category: AI
tags:
- NLP
- transformers
- OpenAI
- GPT2
layout: post
published: false
---
{% include JB/setup %}


### Initial Thoughts

*  Are the results too-good-to-be-true?
   -   Looking at the progression of GPT to BERT to NxBERT 
   -   Blog-posted output is admittedly cherry-picked (according to paper)
   -   PR-oriented outreach via blog, research content not so up-beat
   -   Research report also includes : 
       +   Trivia knowledge over-sold (since only top-30, whereas accuracy is &lt;1% overall)
       +   Main curiosity is about raw performance (eg: summarization via TL;DR)
       +   ...
   
   
*  Does it matter whether results are real?
   -   Is there value in announcing these results (for a debate)?
   -   Is this the purpose of the announcement?
   -   Actual results may be irrelevant?
   -   Watching the debate is v. informative in itself
   -   Notably, researchers from OpenAI are leaving the questions to their Ethics people



*  Why did Elon Musk leave?  And why no fan-fare?
   -   Actually had too much work to do at Tesla
   -   Got more interested in meme wars than future of humanity, etc  /s
   -   'Fake news' experiment might do credibility damage
   -   Actually worried about speed of development
   -   Wanted to use results in his own projects + denied
   -   Culmination of factors, including previous poaching of people
   

<!--
Trivia knowledge show tip-of-iceberg nature of what's being learned.

Per the [Week 8 (part c) CS294-158 Deep Unsupervised Learning (4/3/19) -- Ilya Sutskever](https://www.youtube.com/watch?v=X-B3nAN7YRM) lecture,
some certain number of parameters are required to learn the language 
(sentiment neuron only appears once the base language stuff is taken care of).
And the LMs have a lot of 'noise' that they find very 'attractive' - compared to the much less dense 'high level stuff'.

Basis of new idea : 
Perhaps the higher level stuff should be pulled from a DB rather than memorised within the network weights
(I have a plan...)
... but TransformerXL isn't going to give me decent sent2vec, unfortunately
      so have a look at random methods for sent2vec : https://arxiv.org/pdf/1901.10444.pdf

Useful: 
  *  https://github.com/google-research/bert/issues/276
  *  "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks" : https://arxiv.org/pdf/1903.05987.pdf
  *  "NO TRAINING REQUIRED: EXPLORING RANDOM ENCODERS FOR SENTENCE CLASSIFICATION" : https://arxiv.org/pdf/1901.10444.pdf
  *  (?) Hierarchical Question Answering : ./2_Google_HierarchicalSearchWithBERT_1809.10658.pdf
     - https://arxiv.org/abs/1809.10658
     

  *  Write a beam-searcher - would be much more efficient than plain 'best of 100' current version
     - https://harangdev.github.io/deep-learning/recurrent-neural-networks/43/
     - https://github.com/CongBao/ChatBot/blob/master/chatbot/model.py#L189
     - Interesting : https://distill.pub/2017/ctc/
     - Twitter epic with pointers to updated ideas about Beam Search etc :
         +  https://twitter.com/thom_wolf/status/1124263846674345985
            = Beam bias fix : Correcting Length Bias in Neural Machine Translation 
               ~  https://arxiv.org/abs/1808.10006
               ~  p3 : Add 0.6*n_bpes (32k byte-pair encoding) as correction factor to sum(log_prob)...
            = Nucleus Sampling : The Curious Case of Neural Text Degeneration
               -  https://arxiv.org/abs/1904.09751 
               -  "Figure 1: Beam search leads to degenerate text, even when generated from GPT-2-117M, ..."
               -  "Maxims of Communication (Grice, 1975) have established that people optimize against stating the obvious, making highly predictable text unlikely to occur in practice."

            = Code : https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317
         +  Perhaps 'most popular' wasn't such a bad idea
         
         +  Either way, should incorporate early stopping (again)

  *  Finish creating USE embeddings to see value of supervised pretraining too
     - Universal Sentence Encoder (seems to work, but has supervised training) :
        +  We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015).
     
  *  Add diagram of information flow (would be useful for poster too)
  
  *  Better sentence embedding : 
     - https://github.com/Separius/awesome-sentence-embedding
     - http://nlp.town/blog/sentence-similarity/
        +  https://github.com/henghuiz/SIF_transformer/blob/master/sif_embed/SIF_transformer.py
           =   https://github.com/PrincetonML/SIF
           =   "A Simple but Tough-to-Beat Baseline for Sentence Embeddings"
           =   https://openreview.net/forum?id=SyK00v5xx
        +  https://github.com/peter3125/sentence2vec
           =   https://github.com/vyraun/sentence2vec-1/blob/master/sentence2vec.py
  
  *  Experiment with different 'list of facts' schemes - to see whether it can be honed
     - eg: One sentence (/10) at a time, pick the highest confidence answer
        +  Still need answer probability
        +  Maybe scale it by 1/(len()^alpha)) to do comparisons
        +  However, in our case, shorter is probably better (needs testing...)
        +  *very* time-consuming.  Not clear ranking works at all
        
  *  Eliminate answers that simply repeat the question (similarity>0.5)
     - Need to return list of answers, so that second (or third, etc)-best can fill in
     - Also answers that are contained in the question verbatim
     - Also kill 'yes/no', 'i don't know', 'none' 'no one'
     
Workshop paper submitted to ICML...

----------


Important dates (https://aideadlin.es/?sub=ML,SP,NLP,DM) : 

*  ICML Workshop on Self-Supervised Learning 
   -  https://sites.google.com/view/self-supervised-icml2019
      -  OLD : 
          -  DONE : Submission deadline: April 25, 2019 (Any time)
          -  Notifications: May 10, 2019
          -  Camera Ready: May 31, 2019 (Any time)
          -  Workshop: June 14 or 15, 2019 
      -  UPDATED : 
          -  Submission deadline: May 6, 2019 (Any time) == Tuesday in ~10days 8pm
          -  Notifications: May 13, 2019
          -  DONE!
   -  (ICML conf = Monday, 10 June - Saturday, 15 June)
      +  https://icml.cc/Conferences/2019/Schedule?type=Workshop
      +  Early registration pricing ends : May 19, 2019, 11:59 a.m. 
   - ==SUCCESS! - except found out about acceptance too late to go to LA...
 
*  EMNLP-IJCNLP 2019 November 3-7, 2019. Hong Kong.                        (Abstract due 15-May, Paper due 21-May)
   -  https://www.emnlp-ijcnlp2019.org/calls/papers
   -  https://www.emnlp-ijcnlp2019.org/calls/demos  (due 1-July, including paper, screenshots and screencast explainer)
   
*  NeurIPS 2019 December 9-14, 2019. Vancouver Convention Centre, Canada.  (Abstract due 16-May, Paper due 23-May)


!-->