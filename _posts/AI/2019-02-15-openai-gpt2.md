---
date: 2019-02-15
title:  "OpenAI GPT2"
tagline: Commentary
category: AI
tags:
- NLP
- transformers
- OpenAI
- GPT2
layout: post
published: false
---
{% include JB/setup %}


### Initial Thoughts

*  Are the results too-good-to-be-true?
   -   Looking at the progression of GPT to BERT to NxBERT 
   -   Blog-posted output is admittedly cherry-picked (according to paper)
   -   PR-oriented outreach via blog, research content not so up-beat
   -   Research report also includes : 
       +   Trivia knowledge over-sold (since only top-30, whereas accuracy is &lt;1% overall)
       +   Main curiosity is about raw performance (eg: summarization via TL;DR)
       +   ...
   
   
*  Does it matter whether results are real?
   -   Is there value in announcing these results (for a debate)?
   -   Is this the purpose of the announcement?
   -   Actual results may be irrelevant?
   -   Watching the debate is v. informative in itself
   -   Notably, researchers from OpenAI are leaving the questions to their Ethics people



*  Why did Elon Musk leave?  And why no fan-fare?
   -   Actually had too much work to do at Tesla
   -   Got more interested in meme wars than future of humanity, etc  /s
   -   'Fake news' experiment might do credibility damage
   -   Actually worried about speed of development
   -   Wanted to use results in his own projects + denied
   -   Culmination of factors, including previous poaching of people
   

<!--
Trivia knowledge show tip-of-iceberg nature of what's being learned.

Per the [Week 8 (part c) CS294-158 Deep Unsupervised Learning (4/3/19) -- Ilya Sutskever](https://www.youtube.com/watch?v=X-B3nAN7YRM) lecture,
some certain number of parameters are required to learn the language 
(sentiment neuron only appears once the base language stuff is taken care of).
And the LMs have a lot of 'noise' that they find very 'attractive' - compared to the much less dense 'high level stuff'.

Basis of new idea : 
Perhaps the higher level stuff should be pulled from a DB rather than memorised within the network weights
(I have a plan...)
... but TransformerXL isn't going to give me decent sent2vec, unfortunately
      so have a look at random methods for sent2vec : https://arxiv.org/pdf/1901.10444.pdf

Useful: 
  *  https://github.com/google-research/bert/issues/276
  *  "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks" : https://arxiv.org/pdf/1903.05987.pdf
  *  "NO TRAINING REQUIRED: EXPLORING RANDOM ENCODERS FOR SENTENCE CLASSIFICATION" : https://arxiv.org/pdf/1901.10444.pdf



Important dates (https://aideadlin.es/?sub=ML,SP,NLP,DM) : 

*  (!) ICML Workshop on Self-Supervised Learning 
   -  https://sites.google.com/view/self-supervised-icml2019
   -  (ICML conf = Monday, 10 June - Saturday, 15 June)
      +  https://icml.cc/Conferences/2019/Schedule?type=Workshop

*  EMNLP-IJCNLP 2019 November 3-7, 2019. Hong Kong.                        (Abstract due 15-May, Paper due 21-May)
*  NeurIPS 2019 December 9-14, 2019. Vancouver Convention Centre, Canada.  (Abstract due 16-May, Paper due 23-May)


!-->